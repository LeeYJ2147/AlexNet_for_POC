{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "622685f3",
   "metadata": {},
   "source": [
    "(I only used Gemini to translate the original Korean text.)\n",
    "\n",
    "This experiment was conducted based on the groundwork laid by the Professor's research, specifically referencing the paper **'Hierarchical Deep Feature Fusion and Ensemble Learning for Enhanced Brain Tumor MRI Classification'** (hereafter referred to as the Target Paper).\n",
    "\n",
    "Drawing inspiration from the Target Paper's methodology, we applied the **Machine Learning (ML) Ensemble** approach to the AlexNet architecture. Unlike the Target Paper, which used various Vision Transformer (ViT) models, this experiment utilized **features extracted from different layers of AlexNet**. Specifically, features were extracted from the **last Fully Connected (FC) layer** (the one preceding the final $4096 \\to \\text{num\\_class}$ output layer) and the **last Convolutional Neural Network (CNN) layer**.\n",
    "\n",
    "Initially, we had intended to experiment with a wide array of ML classifiers and optimize each model using a comprehensive range of hyperparameters.\n",
    "\n",
    "However, due to the **limited timeframe of only one to two weeks** and the **concurrent exam period**, it was challenging to dedicate sufficient time to the research. Consequently, we regret that we were unable to conduct thorough analysis of prior work, explore diverse architectural ideas, or perform extensive experiments.\n",
    "\n",
    "The resulting performance was **lower than the SOTA model** found in the Proof-of-Concept (POC) dataset from the Professor's related work, **'Systematic Integration of Attention Modules into CNNs for Accurate and Generalizable Medical Image Diagnosis'**. Had more time been available, we believe we could have conducted more meaningful and impactful experiments.\n",
    "\n",
    "The **best performance** was achieved by the version using the **AlexNet + BN + CBAM** architecture with **10-Crop** data input, and employing **MLP, KNN, & SVM** as the classifiers in the final ensemble. The **test accuracy** achieved in this setup was **0.8623**. This corresponds to the experiment shown in the final block of this Jupyter Notebook (`ipynb`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "126de7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report\n",
    "from src.data_loader import create_dataloader\n",
    "from src.models import create_model\n",
    "\n",
    "\n",
    "def train_and_evaluate_ml_model(X_train, y_train, X_test, y_test, model_name):\n",
    "    model_map = {\n",
    "        'mlp': MLPClassifier(random_state=42, max_iter=1000, \n",
    "                             activation='relu', solver='adam', \n",
    "                             hidden_layer_sizes=(100, 100, 50), learning_rate_init=0.001),\n",
    "                             \n",
    "        'knn': KNeighborsClassifier(n_jobs=4, n_neighbors=7, \n",
    "                                    weights='distance', algorithm='auto'),\n",
    "                                     \n",
    "        'svm_rbf': SVC(probability=True, random_state=42, kernel='rbf', \n",
    "                       C=1.0, gamma='auto') \n",
    "    }\n",
    "\n",
    "    model = model_map[model_name]\n",
    "    \n",
    "    N_train, N_crops_train, D = X_train.shape\n",
    "    X_train_flat = X_train.reshape(-1, D)\n",
    "    y_train_flat = np.repeat(y_train, N_crops_train)\n",
    "\n",
    "    print(f\"\\n--- Training and Evaluating {model_name.upper()} ---\")\n",
    "    \n",
    "    model.fit(X_train_flat, y_train_flat)\n",
    "    cv_scores = cross_val_score(model, X_train_flat, y_train_flat, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    val_accuracy = np.mean(cv_scores)\n",
    "    \n",
    "    print(f\"\\n[RESULTS] Single {model_name.upper()} model:\")\n",
    "    print(f\"  - Mean 5-fold CV Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    N_test, N_crops_test, _ = X_test.shape\n",
    "    X_test_flat = X_test.reshape(-1, D)\n",
    "    \n",
    "    probas_flat = model.predict_proba(X_test_flat)\n",
    "    \n",
    "    Num_classes = probas_flat.shape[-1]\n",
    "    probas_reshaped = probas_flat.reshape(N_test, N_crops_test, Num_classes)\n",
    "    \n",
    "    \n",
    "    probas_mean = probas_reshaped.mean(axis=1)\n",
    "    y_pred = np.argmax(probas_mean, axis=1)\n",
    "    \n",
    "    print(\"  - Test Set Performance:\")\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "    \n",
    "    return {\n",
    "        'name': model_name,\n",
    "        'estimator': model,\n",
    "        'val_score': val_accuracy,\n",
    "        'probas_mean': probas_mean\n",
    "    }\n",
    "\n",
    "def run_ml_experiment_flow(X_train, y_train, X_test, y_test):\n",
    "    model_names = ['mlp', 'knn', 'svm_rbf'] \n",
    "    results = []\n",
    "    \n",
    "    for name in model_names:\n",
    "        result = train_and_evaluate_ml_model(X_train, y_train, X_test, y_test, name)\n",
    "        results.append(result)\n",
    "\n",
    "    weights = []\n",
    "    for r in results:\n",
    "        weights.append(r['val_score'])\n",
    "    \n",
    "    weights = np.array(weights) / np.sum(weights)\n",
    "    print(f\"Normalized weights: {[f'{w:.4f}' for w in weights]}\")\n",
    "\n",
    "    final_probas = np.zeros(results[0]['probas_mean'].shape)\n",
    "\n",
    "    for i, r in enumerate(results):\n",
    "        probas_mean = r['probas_mean']\n",
    "        final_probas += weights[i] * probas_mean\n",
    "    \n",
    "    final_predictions = np.argmax(final_probas, axis=1)\n",
    "\n",
    "    print(\"\\n--- Final Ensemble Test Set Performance ---\")\n",
    "    print(classification_report(y_test, final_predictions, digits=4))\n",
    "    \n",
    "    print(f\"\\nTest Accuracy: {(y_test == final_predictions).sum().item() / len(y_test)}\")\n",
    "\n",
    "def extract_features(model, dataloader, mode, device):\n",
    "    model.eval()\n",
    "    features_list, labels_list = [], []\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader, desc=f\"Extracting features ({mode})\")\n",
    "        for inputs, labels in pbar:\n",
    "            inputs = inputs.to(device)\n",
    "            if len(inputs.shape) == 5:\n",
    "                B, N_crops, C, H, W = inputs.shape\n",
    "                inputs = inputs.view(-1, C, H, W) \n",
    "                features = model(inputs, mode=mode)\n",
    "                Feature_Dim = features.shape[-1]\n",
    "                features = features.view(B, N_crops, Feature_Dim) \n",
    "            else:\n",
    "                features = model(inputs, mode=mode).unsqueeze(1) # (B, 1, Feature_Dim)\n",
    "            features_list.append(features.cpu().numpy())\n",
    "            labels_list.append(labels.cpu().numpy())\n",
    "    return np.concatenate(features_list, axis=0), np.concatenate(labels_list, axis=0)\n",
    "\n",
    "def classifier_ml(config_path):\n",
    "    with open(config_path, 'r') as f:\n",
    "        exp3_config = yaml.safe_load(f)\n",
    "\n",
    "    feature_mode = exp3_config['feature_mode']\n",
    "    yaml_name = exp3_config['feature_extraction_model']['yaml_name']\n",
    "    best_model = exp3_config['feature_extraction_model']['pt_name']\n",
    "    dl_config_path = f\"configs/{yaml_name}.yaml\"\n",
    "    checkpoint_path = f\"saved_models/{best_model}/best_model.pth\"\n",
    "    \n",
    "    print(f\"--- Starting Quick Experiment: Feature Mode '{feature_mode}' ---\")\n",
    "    print(f\"Using DL model from run: '{best_model}'\")\n",
    "\n",
    "    with open(dl_config_path, 'r') as f:\n",
    "        dl_config = yaml.safe_load(f)\n",
    "\n",
    "    device = torch.device(dl_config['training'].get('device', 'cpu'))\n",
    "    model = create_model(\n",
    "        model_name=dl_config['model']['name'],\n",
    "        num_classes=dl_config['data']['num_classes'],\n",
    "        pretrained=dl_config['model']['pretrained'],\n",
    "        **dl_config['model'].get('params', {})\n",
    "    ).to(device)\n",
    "    \n",
    "    try:\n",
    "        model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "        print(f\"Successfully loaded checkpoint: {checkpoint_path}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Checkpoint file not found at '{checkpoint_path}'\")\n",
    "        return\n",
    "\n",
    "    train_loader, test_loader = create_dataloader(dl_config['data'])\n",
    "    X_train, y_train = extract_features(model, train_loader, feature_mode, device)\n",
    "    X_test, y_test = extract_features(model, test_loader, feature_mode, device)\n",
    "    print(f\"Features extracted. Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "\n",
    "    run_ml_experiment_flow(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfa23ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Quick Experiment: Feature Mode 'extract_fc7' ---\n",
      "Using DL model from run: 'exp2_04_10crop_bn+cbam'\n",
      "Model 'alexnet_bn_cbam' created. Pretrained: True, Num classes: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\m\\AppData\\Local\\Temp\\ipykernel_7688\\3129683079.py:136: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded checkpoint: saved_models/exp2_04_10crop_bn+cbam/best_model.pth\n",
      "DataLoaders created successfully.\n",
      "  - Preprocessing: 10crop\n",
      "  - Train samples: 4155, Test samples: 1511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features (extract_fc7): 100%|██████████| 130/130 [00:15<00:00,  8.40it/s]\n",
      "Extracting features (extract_fc7): 100%|██████████| 48/48 [00:19<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted. Train shape: (4155, 1, 4096), Test shape: (1511, 10, 4096)\n",
      "\n",
      "--- Training and Evaluating MLP ---\n",
      "\n",
      "[RESULTS] Single MLP model:\n",
      "  - Mean 5-fold CV Accuracy: 0.9153\n",
      "  - Test Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8819    0.9769    0.9270       390\n",
      "           1     0.6587    0.7135    0.6850       349\n",
      "           2     0.7906    0.7981    0.7943       421\n",
      "           3     0.9928    0.7806    0.8740       351\n",
      "\n",
      "    accuracy                         0.8206      1511\n",
      "   macro avg     0.8310    0.8173    0.8201      1511\n",
      "weighted avg     0.8307    0.8206    0.8218      1511\n",
      "\n",
      "\n",
      "--- Training and Evaluating KNN ---\n",
      "\n",
      "[RESULTS] Single KNN model:\n",
      "  - Mean 5-fold CV Accuracy: 0.9122\n",
      "  - Test Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8991    0.9821    0.9387       390\n",
      "           1     0.8351    0.6676    0.7420       349\n",
      "           2     0.7764    0.9074    0.8368       421\n",
      "           3     0.9873    0.8832    0.9323       351\n",
      "\n",
      "    accuracy                         0.8657      1511\n",
      "   macro avg     0.8745    0.8601    0.8625      1511\n",
      "weighted avg     0.8706    0.8657    0.8634      1511\n",
      "\n",
      "\n",
      "--- Training and Evaluating SVM_RBF ---\n",
      "\n",
      "[RESULTS] Single SVM_RBF model:\n",
      "  - Mean 5-fold CV Accuracy: 0.9259\n",
      "  - Test Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9078    0.9846    0.9446       390\n",
      "           1     0.8121    0.6562    0.7258       349\n",
      "           2     0.7590    0.8979    0.8226       421\n",
      "           3     0.9838    0.8632    0.9196       351\n",
      "\n",
      "    accuracy                         0.8564      1511\n",
      "   macro avg     0.8657    0.8505    0.8532      1511\n",
      "weighted avg     0.8619    0.8564    0.8543      1511\n",
      "\n",
      "Normalized weights: ['0.3324', '0.3313', '0.3363']\n",
      "\n",
      "--- Final Ensemble Test Set Performance ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9033    0.9821    0.9410       390\n",
      "           1     0.7938    0.6619    0.7219       349\n",
      "           2     0.7640    0.9074    0.8295       421\n",
      "           3     0.9932    0.8376    0.9088       351\n",
      "\n",
      "    accuracy                         0.8537      1511\n",
      "   macro avg     0.8636    0.8472    0.8503      1511\n",
      "weighted avg     0.8601    0.8537    0.8519      1511\n",
      "\n",
      "\n",
      "Test Accuracy: 0.8537392455327598\n"
     ]
    }
   ],
   "source": [
    "classifier_ml('configs/exp3_a.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c81ec1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Quick Experiment: Feature Mode 'extract_fused' ---\n",
      "Using DL model from run: 'exp2_04_10crop_bn+cbam'\n",
      "Model 'alexnet_bn_cbam' created. Pretrained: True, Num classes: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\m\\AppData\\Local\\Temp\\ipykernel_7688\\3129683079.py:136: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded checkpoint: saved_models/exp2_04_10crop_bn+cbam/best_model.pth\n",
      "DataLoaders created successfully.\n",
      "  - Preprocessing: 10crop\n",
      "  - Train samples: 4155, Test samples: 1511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features (extract_fused): 100%|██████████| 130/130 [00:15<00:00,  8.38it/s]\n",
      "Extracting features (extract_fused): 100%|██████████| 48/48 [00:20<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extracted. Train shape: (4155, 1, 4352), Test shape: (1511, 10, 4352)\n",
      "\n",
      "--- Training and Evaluating MLP ---\n",
      "\n",
      "[RESULTS] Single MLP model:\n",
      "  - Mean 5-fold CV Accuracy: 0.9208\n",
      "  - Test Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8991    0.9821    0.9387       390\n",
      "           1     0.7597    0.6160    0.6804       349\n",
      "           2     0.7696    0.8646    0.8143       421\n",
      "           3     0.9331    0.8746    0.9029       351\n",
      "\n",
      "    accuracy                         0.8398      1511\n",
      "   macro avg     0.8404    0.8343    0.8341      1511\n",
      "weighted avg     0.8387    0.8398    0.8361      1511\n",
      "\n",
      "\n",
      "--- Training and Evaluating KNN ---\n",
      "\n",
      "[RESULTS] Single KNN model:\n",
      "  - Mean 5-fold CV Accuracy: 0.9148\n",
      "  - Test Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9054    0.9821    0.9422       390\n",
      "           1     0.8172    0.6533    0.7261       349\n",
      "           2     0.7658    0.8931    0.8246       421\n",
      "           3     0.9748    0.8832    0.9268       351\n",
      "\n",
      "    accuracy                         0.8584      1511\n",
      "   macro avg     0.8658    0.8529    0.8549      1511\n",
      "weighted avg     0.8623    0.8584    0.8559      1511\n",
      "\n",
      "\n",
      "--- Training and Evaluating SVM_RBF ---\n",
      "\n",
      "[RESULTS] Single SVM_RBF model:\n",
      "  - Mean 5-fold CV Accuracy: 0.9230\n",
      "  - Test Set Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9121    0.9846    0.9470       390\n",
      "           1     0.8237    0.6562    0.7305       349\n",
      "           2     0.7652    0.8979    0.8262       421\n",
      "           3     0.9843    0.8917    0.9357       351\n",
      "\n",
      "    accuracy                         0.8630      1511\n",
      "   macro avg     0.8713    0.8576    0.8598      1511\n",
      "weighted avg     0.8675    0.8630    0.8607      1511\n",
      "\n",
      "Normalized weights: ['0.3338', '0.3316', '0.3346']\n",
      "\n",
      "--- Final Ensemble Test Set Performance ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9057    0.9846    0.9435       390\n",
      "           1     0.8364    0.6447    0.7282       349\n",
      "           2     0.7680    0.9121    0.8339       421\n",
      "           3     0.9748    0.8832    0.9268       351\n",
      "\n",
      "    accuracy                         0.8623      1511\n",
      "   macro avg     0.8712    0.8562    0.8581      1511\n",
      "weighted avg     0.8674    0.8623    0.8593      1511\n",
      "\n",
      "\n",
      "Test Accuracy: 0.8623428193249504\n"
     ]
    }
   ],
   "source": [
    "classifier_ml('configs/exp3_b.yaml')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trading",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
